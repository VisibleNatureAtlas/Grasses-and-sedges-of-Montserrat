{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Generate multilingual taxa pages with OSM maps\n\nThis notebook generates enriched, multilingual species pages for the Visible Nature Atlas.\n\n**For each species, it:**\n1. Fetches worldwide GBIF occurrence records (with coordinates)\n2. Generates an interactive Leaflet/OpenStreetMap map\n3. Discovers available Wikipedia language editions via Wikidata SPARQL\n4. Fetches Wikipedia introductions **per language**\n5. Fetches Plazi TreatmentBank treatments and BHL literature\n6. Generates a **Wikipedia language availability table** (which species has articles in which languages)\n7. Writes complete markdown pages per language: `taxa_{lang}/Genus_species.md`\n8. Generates per-language `_config_{lang}.yml`, `_toc_{lang}.yml`, `intro_{lang}.md`\n9. Creates a landing page (`index.html`) with a language picker\n\n**Prerequisites**: Run `inspectData.ipynb` first to generate `gbifMontserrat.ttl`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport time\nimport json\nimport requests\nimport pandas as pd\nimport folium\nimport yaml\nimport tqdm\nimport colorsys\nfrom rdflib import Graph, Namespace\nfrom rdflib.namespace import RDFS\n\nWDT = Namespace(\"http://www.wikidata.org/prop/direct/\")\nWD  = Namespace(\"http://www.wikidata.org/entity/\")\n\n# BHL API key — set via environment variable or paste here for local testing\nBHL_API_KEY = os.environ.get('BHL_API_KEY', '')\n\n# Bot-generated Wikipedia editions to exclude (thin/auto-generated content)\nBOT_WIKIS = {'ceb', 'war', 'min', 'shn'}\n\n# Minimum number of species that must have a Wikipedia article in a language\n# for that language to get its own book\nMIN_SPECIES_PER_LANG = 3\n\nos.makedirs('maps', exist_ok=True)\n\nif BHL_API_KEY:\n    print(f'BHL API key loaded ({len(BHL_API_KEY)} chars).')\nelse:\n    print('No BHL_API_KEY set — BHL sections will be skipped.')\n\nprint('Dependencies loaded.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load the local RDF graph\n\nWe load `gbifMontserrat.ttl` generated by `inspectData.ipynb` to get the list of species and their Wikidata URIs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.parse('gbifMontserrat.ttl', format='turtle')\n",
    "\n",
    "# Extract distinct taxa: {taxon_label: wikidata_qid}\n",
    "taxa_query = \"\"\"\n",
    "PREFIX wd:  <http://www.wikidata.org/entity/>\n",
    "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT DISTINCT ?taxon ?taxonLabel WHERE {\n",
    "    ?obs wdt:P225 ?taxon .\n",
    "    ?taxon rdfs:label ?taxonLabel .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "taxa = {}\n",
    "for row in g.query(taxa_query):\n",
    "    qid = str(row.taxon).replace('http://www.wikidata.org/entity/', '')\n",
    "    label = str(row.taxonLabel)\n",
    "    taxa[label] = {'qid': qid, 'uri': str(row.taxon)}\n",
    "\n",
    "print(f'Found {len(taxa)} distinct taxa:')\n",
    "for name, info in sorted(taxa.items()):\n",
    "    print(f'  {name} ({info[\"qid\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Helper functions\n\nAll data-fetching and utility functions: GBIF occurrences, Wikipedia intros (multilingual),\nlicense resolution, Plazi treatments, BHL publications, and **Wikidata language discovery**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gbif_occurrences(taxon_name, limit=300):\n",
    "    \"\"\"Fetch georeferenced GBIF occurrences for a species by name.\"\"\"\n",
    "    url = 'https://api.gbif.org/v1/occurrence/search'\n",
    "    params = {\n",
    "        'scientificName': taxon_name,\n",
    "        'hasCoordinate': 'true',\n",
    "        'hasGeospatialIssue': 'false',\n",
    "        'limit': limit,\n",
    "        'offset': 0\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(url, params=params, timeout=20)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        results = []\n",
    "        for rec in data.get('results', []):\n",
    "            lat = rec.get('decimalLatitude')\n",
    "            lon = rec.get('decimalLongitude')\n",
    "            if lat is not None and lon is not None:\n",
    "                results.append({\n",
    "                    'lat': lat, 'lon': lon,\n",
    "                    'country': rec.get('country', ''),\n",
    "                    'year': rec.get('year', ''),\n",
    "                    'institution': rec.get('institutionCode', ''),\n",
    "                    'gbifID': rec.get('gbifID', ''),\n",
    "                    'basisOfRecord': rec.get('basisOfRecord', '')\n",
    "                })\n",
    "        return results, data.get('count', 0)\n",
    "    except Exception as e:\n",
    "        print(f'  Warning: could not fetch GBIF occurrences for {taxon_name}: {e}')\n",
    "        return [], 0\n",
    "\n",
    "\n",
    "def get_wikipedia_intro(taxon_name, lang='en'):\n",
    "    \"\"\"Fetch the introductory paragraph from Wikipedia in the given language.\"\"\"\n",
    "    url = f'https://{lang}.wikipedia.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'query', 'format': 'json',\n",
    "        'titles': taxon_name.replace(' ', '_'),\n",
    "        'prop': 'extracts', 'exintro': True,\n",
    "        'explaintext': True, 'redirects': 1\n",
    "    }\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, timeout=10)\n",
    "            if resp.status_code != 200:\n",
    "                time.sleep(1 * (attempt + 1))\n",
    "                continue\n",
    "            data = resp.json()\n",
    "            page = next(iter(data['query']['pages'].values()))\n",
    "            extract = page.get('extract', '').strip()\n",
    "            paragraphs = [p.strip() for p in extract.split('\\n') if p.strip()]\n",
    "            return paragraphs[0] if paragraphs else ''\n",
    "        except Exception:\n",
    "            time.sleep(1 * (attempt + 1))\n",
    "    return ''\n",
    "\n",
    "\n",
    "def resolve_license_label(license_uri):\n",
    "    \"\"\"Convert a Wikidata license URI to a (label, url) tuple.\"\"\"\n",
    "    license_map = {\n",
    "        'Q18199165': ('CC BY 4.0',       'https://creativecommons.org/licenses/by/4.0/'),\n",
    "        'Q20007257': ('CC BY-SA 4.0',    'https://creativecommons.org/licenses/by-sa/4.0/'),\n",
    "        'Q6938433':  ('CC0 1.0',         'https://creativecommons.org/publicdomain/zero/1.0/'),\n",
    "        'Q19068220': ('CC BY-NC 4.0',    'https://creativecommons.org/licenses/by-nc/4.0/'),\n",
    "        'Q26952697': ('CC BY-NC-SA 4.0', 'https://creativecommons.org/licenses/by-nc-sa/4.0/'),\n",
    "        'Q35254':    ('CC BY-SA 3.0',    'https://creativecommons.org/licenses/by-sa/3.0/'),\n",
    "        'Q24082749': ('CC BY-SA 4.0',    'https://creativecommons.org/licenses/by-sa/4.0/'),\n",
    "    }\n",
    "    s = str(license_uri)\n",
    "    if 'wikidata.org/entity/Q' in s:\n",
    "        qid = s.split('/')[-1]\n",
    "        if qid in license_map:\n",
    "            return license_map[qid]\n",
    "        return qid, s\n",
    "    s_lower = s.lower()\n",
    "    if 'publicdomain/zero' in s_lower:   return 'CC0 1.0', s\n",
    "    if 'by-nc-sa' in s_lower:           return 'CC BY-NC-SA 4.0', s\n",
    "    if 'by-nc' in s_lower:              return 'CC BY-NC 4.0', s\n",
    "    if 'by-sa' in s_lower:              return 'CC BY-SA 4.0', s\n",
    "    if 'by' in s_lower:                 return 'CC BY 4.0', s\n",
    "    return s, s\n",
    "\n",
    "\n",
    "def fetch_plazi_treatments(genus, species):\n",
    "    \"\"\"Fetch taxonomic treatments from Plazi via LINDAS SPARQL endpoint.\"\"\"\n",
    "    query = '''\n",
    "PREFIX dwc: <http://rs.tdwg.org/dwc/terms/>\n",
    "PREFIX dc:  <http://purl.org/dc/elements/1.1/>\n",
    "PREFIX treat: <http://plazi.org/vocab/treatment#>\n",
    "\n",
    "SELECT DISTINCT ?treatment ?creator\n",
    "       (SAMPLE(?title) AS ?pubTitle)\n",
    "WHERE {{\n",
    "  ?treatment a treat:Treatment .\n",
    "  ?treatment (treat:deprecates|treat:augmentsTaxonConcept|treat:definesTaxonConcept) ?tc .\n",
    "  ?tc dwc:species \"{sp}\" .\n",
    "  ?tc dwc:genus \"{ge}\" .\n",
    "  OPTIONAL {{ ?treatment dc:creator ?creator . }}\n",
    "  OPTIONAL {{ ?treatment treat:publishedIn ?pub . ?pub dc:title ?title . }}\n",
    "}}\n",
    "GROUP BY ?treatment ?creator\n",
    "LIMIT 10\n",
    "'''.format(ge=genus, sp=species)\n",
    "    try:\n",
    "        resp = requests.get(\n",
    "            'https://lindas.admin.ch/query',\n",
    "            params={'query': query},\n",
    "            headers={'Accept': 'application/sparql-results+json',\n",
    "                     'User-Agent': 'VisibleNatureAtlas/1.0'},\n",
    "            timeout=20\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        results = []\n",
    "        seen = set()\n",
    "        for b in data.get('results', {}).get('bindings', []):\n",
    "            t_url = b.get('treatment', {}).get('value', '')\n",
    "            if t_url in seen:\n",
    "                continue\n",
    "            seen.add(t_url)\n",
    "            results.append({\n",
    "                'treatment_url': t_url,\n",
    "                'treatment_page': t_url.replace('http://', 'https://'),\n",
    "                'creator': b.get('creator', {}).get('value', ''),\n",
    "                'pub_title': b.get('pubTitle', {}).get('value', ''),\n",
    "            })\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f'  Plazi warning for {genus} {species}: {e}')\n",
    "        return []\n",
    "\n",
    "\n",
    "def fetch_bhl_publications(taxon_name, api_key, limit=5):\n",
    "    \"\"\"Search BHL for publications mentioning a taxon.\"\"\"\n",
    "    if not api_key:\n",
    "        print(f'  BHL: skipping {taxon_name} — no API key')\n",
    "        return []\n",
    "    try:\n",
    "        resp = requests.get(\n",
    "            'https://www.biodiversitylibrary.org/api3',\n",
    "            params={\n",
    "                'op': 'PublicationSearch', 'searchterm': taxon_name,\n",
    "                'searchtype': '', 'page': 1, 'pagesize': limit,\n",
    "                'apikey': api_key, 'format': 'json',\n",
    "            },\n",
    "            timeout=15\n",
    "        )\n",
    "        print(f'  BHL HTTP {resp.status_code} for {taxon_name}')\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        status = data.get('Status', '(missing)')\n",
    "        error_msg = data.get('ErrorMessage', '')\n",
    "        result_raw = data.get('Result', [])\n",
    "        n_raw = len(result_raw) if isinstance(result_raw, list) else f'type={type(result_raw).__name__}'\n",
    "        print(f'  BHL response: Status={status}, ErrorMessage={error_msg}, Result count={n_raw}')\n",
    "        if status != 'ok':\n",
    "            print(f'  BHL: Status is not \"ok\" — returning empty for {taxon_name}')\n",
    "            return []\n",
    "        results = []\n",
    "        for item in result_raw[:limit]:\n",
    "            if not isinstance(item, dict):\n",
    "                print(f'  BHL: unexpected Result item type: {type(item).__name__} = {repr(item)[:100]}')\n",
    "                continue\n",
    "            pub_url = item.get('Url', '')\n",
    "            if not pub_url and item.get('TitleID'):\n",
    "                pub_url = 'https://www.biodiversitylibrary.org/title/' + str(item['TitleID'])\n",
    "            results.append({\n",
    "                'title': item.get('Title', 'Untitled'),\n",
    "                'authors': item.get('Authors', ''),\n",
    "                'date': item.get('Date', ''),\n",
    "                'bhl_url': pub_url,\n",
    "            })\n",
    "        print(f'  BHL: {len(results)} publications found for {taxon_name}')\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f'  BHL error for {taxon_name}: {type(e).__name__}: {e}')\n",
    "        return []\n",
    "\n",
    "\n",
    "def discover_wikipedia_languages(qids):\n",
    "    \"\"\"Discover available Wikipedia editions for a set of Wikidata items.\n",
    "    \n",
    "    Uses Wikidata SPARQL to find all sitelinks in one query.\n",
    "    Returns dict: {taxon_label: {lang_code: article_title, ...}, ...}\n",
    "    and a reverse map: {lang_code: set of taxon_labels with articles}\n",
    "    \"\"\"\n",
    "    # Build VALUES clause with QIDs\n",
    "    values = ' '.join(f'wd:{qid}' for qid in qids.values())\n",
    "    \n",
    "    query = f'''\n",
    "SELECT ?item ?itemLabel ?sitelink ?site WHERE {{\n",
    "  VALUES ?item {{ {values} }}\n",
    "  ?sitelink schema:about ?item ;\n",
    "            schema:isPartOf ?site ;\n",
    "            schema:name ?name .\n",
    "  ?site wikibase:wikiGroup \"wikipedia\" .\n",
    "  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\" . }}\n",
    "}}\n",
    "'''\n",
    "    try:\n",
    "        resp = requests.get(\n",
    "            'https://query.wikidata.org/sparql',\n",
    "            params={'query': query, 'format': 'json'},\n",
    "            headers={'User-Agent': 'VisibleNatureAtlas/1.0 (https://github.com/VisibleNatureAtlas)'},\n",
    "            timeout=30\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "    except Exception as e:\n",
    "        print(f'Warning: Wikidata language discovery failed: {e}')\n",
    "        # Fallback: just return English\n",
    "        return {label: {'en': label.replace(' ', '_')} for label in qids.keys()}, {'en': set(qids.keys())}\n",
    "    \n",
    "    # Build QID → label reverse lookup\n",
    "    qid_to_label = {v: k for k, v in qids.items()}\n",
    "    \n",
    "    # Parse results\n",
    "    taxon_langs = {}  # {taxon_label: {lang: article_title}}\n",
    "    lang_taxa = {}    # {lang: set(taxon_labels)}\n",
    "    \n",
    "    for b in data.get('results', {}).get('bindings', []):\n",
    "        item_uri = b.get('item', {}).get('value', '')\n",
    "        qid = item_uri.split('/')[-1]\n",
    "        taxon_label = qid_to_label.get(qid, '')\n",
    "        if not taxon_label:\n",
    "            continue\n",
    "        \n",
    "        site_url = b.get('site', {}).get('value', '')\n",
    "        # Extract language code from site URL like https://en.wikipedia.org/\n",
    "        if '.wikipedia.org' not in site_url:\n",
    "            continue\n",
    "        lang = site_url.split('//')[1].split('.')[0] if '//' in site_url else ''\n",
    "        if not lang or lang in BOT_WIKIS:\n",
    "            continue\n",
    "        \n",
    "        sitelink_url = b.get('sitelink', {}).get('value', '')\n",
    "        article_title = sitelink_url.split('/wiki/')[-1] if '/wiki/' in sitelink_url else taxon_label.replace(' ', '_')\n",
    "        \n",
    "        if taxon_label not in taxon_langs:\n",
    "            taxon_langs[taxon_label] = {}\n",
    "        taxon_langs[taxon_label][lang] = article_title\n",
    "        \n",
    "        if lang not in lang_taxa:\n",
    "            lang_taxa[lang] = set()\n",
    "        lang_taxa[lang].add(taxon_label)\n",
    "    \n",
    "    return taxon_langs, lang_taxa\n",
    "\n",
    "\n",
    "# Language names for display (ISO 639-1 → English name)\n",
    "LANG_NAMES = {\n",
    "    'en': 'English', 'fr': 'French', 'de': 'German', 'es': 'Spanish',\n",
    "    'pt': 'Portuguese', 'it': 'Italian', 'nl': 'Dutch', 'sv': 'Swedish',\n",
    "    'pl': 'Polish', 'ru': 'Russian', 'ja': 'Japanese', 'zh': 'Chinese',\n",
    "    'ko': 'Korean', 'ar': 'Arabic', 'ca': 'Catalan', 'cs': 'Czech',\n",
    "    'da': 'Danish', 'fi': 'Finnish', 'el': 'Greek', 'he': 'Hebrew',\n",
    "    'hi': 'Hindi', 'hu': 'Hungarian', 'id': 'Indonesian', 'ms': 'Malay',\n",
    "    'no': 'Norwegian', 'fa': 'Persian', 'ro': 'Romanian', 'sk': 'Slovak',\n",
    "    'th': 'Thai', 'tr': 'Turkish', 'uk': 'Ukrainian', 'vi': 'Vietnamese',\n",
    "    'eu': 'Basque', 'gl': 'Galician', 'hr': 'Croatian', 'lt': 'Lithuanian',\n",
    "    'lv': 'Latvian', 'sr': 'Serbian', 'sl': 'Slovenian', 'bg': 'Bulgarian',\n",
    "    'et': 'Estonian', 'simple': 'Simple English', 'nb': 'Norwegian Bokm\\u00e5l',\n",
    "    'nn': 'Norwegian Nynorsk', 'eo': 'Esperanto', 'az': 'Azerbaijani',\n",
    "    'ta': 'Tamil', 'te': 'Telugu', 'bn': 'Bengali', 'ur': 'Urdu',\n",
    "    'pa': 'Punjabi', 'ml': 'Malayalam', 'kn': 'Kannada', 'gu': 'Gujarati',\n",
    "    'mr': 'Marathi', 'af': 'Afrikaans', 'sw': 'Swahili', 'ga': 'Irish',\n",
    "    'cy': 'Welsh', 'la': 'Latin', 'sh': 'Serbo-Croatian', 'ast': 'Asturian',\n",
    "    'oc': 'Occitan', 'an': 'Aragonese', 'mg': 'Malagasy', 'tl': 'Tagalog',\n",
    "    'qu': 'Quechua', 'nds': 'Low German',\n",
    "}\n",
    "\n",
    "\n",
    "def get_lang_name(code):\n",
    "    \"\"\"Return the English name for a language code, or the code itself.\"\"\"\n",
    "    return LANG_NAMES.get(code, code)\n",
    "\n",
    "\n",
    "print('All helper functions defined (including multilingual + Plazi + BHL).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Folium map for a species\n",
    "\n",
    "Creates an interactive OpenStreetMap map with clustered observation markers.\n",
    "Returns the path to the saved HTML file (embedded in the markdown page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_species_map(taxon_name, occurrences, output_path):\n",
    "    \"\"\"Generate a Folium map with observation points and save as HTML.\n",
    "    \n",
    "    Args:\n",
    "        taxon_name: Scientific name of the species\n",
    "        occurrences: List of occurrence dicts with lat, lon, country, year, institution, gbifID\n",
    "        output_path: Path to save the HTML map file\n",
    "    Returns:\n",
    "        Path to the saved HTML file, or None if no occurrences\n",
    "    \"\"\"\n",
    "    if not occurrences:\n",
    "        return None\n",
    "\n",
    "    # Compute centroid for initial map view\n",
    "    lats = [o['lat'] for o in occurrences]\n",
    "    lons = [o['lon'] for o in occurrences]\n",
    "    center = [sum(lats) / len(lats), sum(lons) / len(lons)]\n",
    "\n",
    "    m = folium.Map(\n",
    "        location=center,\n",
    "        zoom_start=3,\n",
    "        tiles='OpenStreetMap',\n",
    "        width='100%',\n",
    "        height='450px'\n",
    "    )\n",
    "\n",
    "    # Add tile layer attribution\n",
    "    folium.TileLayer(\n",
    "        tiles='https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png',\n",
    "        attr='© <a href=\"https://www.openstreetmap.org/copyright\">OpenStreetMap</a> contributors',\n",
    "        name='OpenStreetMap',\n",
    "        overlay=False,\n",
    "        control=True\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Colour-code by basis of record\n",
    "    color_map = {\n",
    "        'HUMAN_OBSERVATION': '#2ecc71',\n",
    "        'PRESERVED_SPECIMEN': '#e67e22',\n",
    "        'MACHINE_OBSERVATION': '#3498db',\n",
    "        'LITERATURE': '#9b59b6',\n",
    "        'MATERIAL_SAMPLE': '#e74c3c',\n",
    "    }\n",
    "\n",
    "    for occ in occurrences:\n",
    "        color = color_map.get(occ.get('basisOfRecord', ''), '#95a5a6')\n",
    "        year_str = f\", {occ['year']}\" if occ.get('year') else ''\n",
    "        inst_str = f\"<br><em>{occ['institution']}</em>\" if occ.get('institution') else ''\n",
    "        gbif_link = ''\n",
    "        if occ.get('gbifID'):\n",
    "            gbif_link = f'<br><a href=\"https://www.gbif.org/occurrence/{occ[\"gbifID\"]}\" target=\"_blank\">GBIF record</a>'\n",
    "        \n",
    "        popup_html = (\n",
    "            f\"<b>{taxon_name}</b><br>\"\n",
    "            f\"{occ.get('country', '')}{year_str}\"\n",
    "            f\"{inst_str}\"\n",
    "            f\"{gbif_link}\"\n",
    "        )\n",
    "        folium.CircleMarker(\n",
    "            location=[occ['lat'], occ['lon']],\n",
    "            radius=5,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            fill_opacity=0.7,\n",
    "            popup=folium.Popup(popup_html, max_width=250),\n",
    "            tooltip=f\"{occ.get('country', '')} {year_str}\"\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Legend\n",
    "    legend_html = '''\n",
    "    <div style=\"position:fixed; bottom:20px; left:20px; z-index:9999;\n",
    "                background:white; padding:10px; border-radius:6px;\n",
    "                border:1px solid #ccc; font-size:12px; line-height:1.8;\">\n",
    "        <b>Basis of record</b><br>\n",
    "        <span style=\"color:#2ecc71;\">●</span> Human observation<br>\n",
    "        <span style=\"color:#e67e22;\">●</span> Preserved specimen<br>\n",
    "        <span style=\"color:#3498db;\">●</span> Machine observation<br>\n",
    "        <span style=\"color:#9b59b6;\">●</span> Literature<br>\n",
    "        <span style=\"color:#95a5a6;\">●</span> Other\n",
    "    </div>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "    m.save(output_path)\n",
    "    return output_path\n",
    "\n",
    "\n",
    "print('Map generation function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load taxa information from the RDF graph\n\nExtract taxon data from `gbifMontserrat.ttl` — observations grouped by publisher,\nwith media URLs and license information."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the local RDF graph for all observation data\n",
    "obs_query = \"\"\"\n",
    "PREFIX wd:  <http://www.wikidata.org/entity/>\n",
    "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX dc:   <http://purl.org/dc/elements/1.1/>\n",
    "\n",
    "SELECT DISTINCT ?taxon ?taxonLabel ?publisher ?publisherLabel \n",
    "                ?observation ?gbifObservation ?media_url ?license WHERE {\n",
    "    ?media wdt:P2699 ?media_url ;\n",
    "           wdt:P275 ?license ;\n",
    "           wdt:P361 ?observation .\n",
    "    ?observation wdt:P225 ?taxon ;\n",
    "                 wdt:P854 ?gbifObservation ;\n",
    "                 wdt:P123 ?publisher .\n",
    "    ?taxon rdfs:label ?taxonLabel .\n",
    "    ?publisher rdfs:label ?publisherLabel .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Build taxonpages: {taxon_label: {qid, publisher: {org: [{obs_id, media, license}]}}}\n",
    "taxonpages = {}\n",
    "\n",
    "for row in g.query(obs_query):\n",
    "    taxon_label = str(row.taxonLabel)\n",
    "    qid = str(row.taxon).replace('http://www.wikidata.org/entity/', '')\n",
    "    publisher = str(row.publisherLabel)\n",
    "    obs_id = str(row.gbifObservation)\n",
    "    media_url = str(row.media_url)\n",
    "    license_uri = str(row.license)\n",
    "\n",
    "    if taxon_label not in taxonpages:\n",
    "        taxonpages[taxon_label] = {'qid': qid, 'uri': str(row.taxon), 'publishers': {}}\n",
    "    \n",
    "    if publisher not in taxonpages[taxon_label]['publishers']:\n",
    "        taxonpages[taxon_label]['publishers'][publisher] = {}\n",
    "    \n",
    "    if obs_id not in taxonpages[taxon_label]['publishers'][publisher]:\n",
    "        taxonpages[taxon_label]['publishers'][publisher][obs_id] = {\n",
    "            'obs_id': obs_id,\n",
    "            'media': [],\n",
    "            'license': license_uri\n",
    "        }\n",
    "    \n",
    "    if media_url and media_url != 'nan':\n",
    "        existing = taxonpages[taxon_label]['publishers'][publisher][obs_id]['media']\n",
    "        if media_url not in existing:\n",
    "            existing.append(media_url)\n",
    "\n",
    "print(f'Loaded {len(taxonpages)} taxa with observation data.')\n",
    "for name in sorted(taxonpages.keys()):\n",
    "    n_obs = sum(len(obs) for obs in taxonpages[name]['publishers'].values())\n",
    "    print(f'  {name}: {len(taxonpages[name][\"publishers\"])} publishers, {n_obs} observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Discover available Wikipedia languages\n\nQuery Wikidata SPARQL for all Wikipedia sitelinks of our species. This determines\nwhich languages will get their own Jupyter Book edition."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build QID map: {taxon_label: QID}\nqid_map = {label: info['qid'] for label, info in taxa.items()}\n\nprint(f'Discovering Wikipedia language editions for {len(qid_map)} taxa via Wikidata SPARQL...')\ntaxon_langs, lang_taxa = discover_wikipedia_languages(qid_map)\n\n# Filter languages with enough species coverage\nactive_langs = {\n    lang: species_set\n    for lang, species_set in lang_taxa.items()\n    if len(species_set) >= MIN_SPECIES_PER_LANG\n}\n\n# Always include English even if it falls below threshold\nif 'en' not in active_langs and 'en' in lang_taxa:\n    active_langs['en'] = lang_taxa['en']\n\nprint(f'\\nWikipedia language availability:')\nfor lang in sorted(active_langs.keys(), key=lambda l: len(active_langs[l]), reverse=True):\n    n = len(active_langs[lang])\n    print(f'  {lang:>8s} ({get_lang_name(lang):>20s}): {n}/{len(taxonpages)} species')\n\nprint(f'\\n{len(active_langs)} languages with >= {MIN_SPECIES_PER_LANG} species articles (+ English).')\nprint(f'Filtered out bot wikis: {BOT_WIKIS}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Fetch language-independent data (GBIF, maps, Plazi, BHL)\n\nThese are fetched once — they don't depend on language. We store the results\nin `shared_data[taxon_name]` for reuse across all language editions."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "shared_data = {}\n",
    "\n",
    "for taxon_name in tqdm.tqdm(sorted(taxonpages.keys()), desc='Fetching shared data'):\n",
    "    info = taxonpages[taxon_name]\n",
    "    qid = info['qid']\n",
    "    safe_name = taxon_name.replace(' ', '_')\n",
    "    \n",
    "    # GBIF occurrences\n",
    "    occurrences, total_count = fetch_gbif_occurrences(taxon_name, limit=300)\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Generate map\n",
    "    map_path = 'maps/' + safe_name + '.html'\n",
    "    generate_species_map(taxon_name, occurrences, map_path)\n",
    "    \n",
    "    # Plazi treatments\n",
    "    parts = taxon_name.split(' ', 1)\n",
    "    genus = parts[0]\n",
    "    species = parts[1] if len(parts) > 1 else ''\n",
    "    treatments = fetch_plazi_treatments(genus, species) if species else []\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    # BHL publications\n",
    "    bhl_pubs = fetch_bhl_publications(taxon_name, BHL_API_KEY)\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    shared_data[taxon_name] = {\n",
    "        'qid': qid,\n",
    "        'safe_name': safe_name,\n",
    "        'occurrences': occurrences,\n",
    "        'total_count': total_count,\n",
    "        'treatments': treatments,\n",
    "        'bhl_pubs': bhl_pubs,\n",
    "        'map_path': map_path,\n",
    "    }\n",
    "\n",
    "# Summary of BHL results\n",
    "bhl_total = sum(1 for sd in shared_data.values() if sd['bhl_pubs'])\n",
    "bhl_pubs_total = sum(len(sd['bhl_pubs']) for sd in shared_data.values())\n",
    "print(f'\\nShared data fetched for {len(shared_data)} species.')\n",
    "print(f'BHL summary: {bhl_total}/{len(shared_data)} species have BHL publications ({bhl_pubs_total} total)')\n",
    "for tn, sd in sorted(shared_data.items()):\n",
    "    n_bhl = len(sd['bhl_pubs'])\n",
    "    if n_bhl > 0:\n",
    "        print(f'  {tn}: {n_bhl} BHL publications')\n",
    "    else:\n",
    "        print(f'  {tn}: no BHL publications')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- Generate Wikipedia language availability table ----\n# This table shows which species have Wikipedia articles in which languages.\n# It will be included in the intro page of each language edition.\n\ndef generate_wikipedia_lang_table(taxon_langs, active_langs, all_taxa):\n    \"\"\"Generate a markdown table showing Wikipedia coverage per species per language.\n    \n    Rows = species (sorted), Columns = active languages (sorted by coverage).\n    Cells = checkmark or dash.\n    Returns markdown string.\n    \"\"\"\n    # Sort languages by coverage (descending), then alphabetically\n    sorted_langs = sorted(\n        active_langs.keys(),\n        key=lambda l: (-len(active_langs[l]), l)\n    )\n    sorted_taxa = sorted(all_taxa)\n    \n    # Header row\n    header = '| Species |'\n    separator = '|---------|'\n    for lang in sorted_langs:\n        name = get_lang_name(lang)\n        header += f' {name} ({lang}) |'\n        separator += ':-:|'\n    \n    rows = [header, separator]\n    \n    for taxon in sorted_taxa:\n        langs_for_taxon = taxon_langs.get(taxon, {})\n        row = f'| *{taxon}* |'\n        for lang in sorted_langs:\n            if lang in langs_for_taxon:\n                # Link to Wikipedia article\n                article = langs_for_taxon[lang]\n                wp_url = f'https://{lang}.wikipedia.org/wiki/{article}'\n                row += f' [{lang}]({wp_url}) |'\n            else:\n                row += ' — |'\n        rows.append(row)\n    \n    return '\\n'.join(rows)\n\n\nwiki_table_md = generate_wikipedia_lang_table(taxon_langs, active_langs, taxonpages.keys())\nprint('Wikipedia language availability table generated.')\nprint(f'Table: {len(taxonpages)} species x {len(active_langs)} languages')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---- Pre-fetch all Wikipedia intros across all languages ----\n# Fetch once, store in wiki_intros[lang][taxon_name] = intro_text\n# This avoids repeated fetches and reduces total API calls\n\nwiki_intros = {}  # {lang: {taxon_name: intro_text}}\n\ntotal_fetches = sum(\n    1 for lang in active_langs\n    for tn in taxonpages\n    if lang in taxon_langs.get(tn, {})\n)\nprint(f'Pre-fetching {total_fetches} Wikipedia intros across {len(active_langs)} languages...')\n\nfetch_count = 0\nfor lang in tqdm.tqdm(sorted(active_langs.keys()), desc='Fetching Wikipedia intros'):\n    wiki_intros[lang] = {}\n    \n    for taxon_name in sorted(taxonpages.keys()):\n        if lang not in taxon_langs.get(taxon_name, {}):\n            continue\n        \n        article_title = taxon_langs[taxon_name][lang]\n        intro = get_wikipedia_intro(article_title, lang=lang)\n        wiki_intros[lang][taxon_name] = intro\n        fetch_count += 1\n        \n        # Polite rate limiting: short sleep every 10 requests\n        if fetch_count % 10 == 0:\n            time.sleep(0.5)\n\nprint(f'Fetched {fetch_count} Wikipedia intros.')"
  },
  {
   "cell_type": "code",
   "id": "kirm0ldjver",
   "source": "# ---- Step 3: Generate per-language taxa pages (using pre-fetched intros) ----\n\nfor lang in tqdm.tqdm(sorted(active_langs.keys()), desc='Generating language editions'):\n    lang_dir = f'taxa_{lang}'\n    os.makedirs(lang_dir, exist_ok=True)\n    lang_name = get_lang_name(lang)\n    \n    print(f'\\n--- {lang_name} ({lang}) ---')\n    \n    toc_taxa_lang = []\n    \n    for taxon_name in sorted(taxonpages.keys()):\n        info = taxonpages[taxon_name]\n        sd = shared_data[taxon_name]\n        qid = sd['qid']\n        safe_name = sd['safe_name']\n        occurrences = sd['occurrences']\n        total_count = sd['total_count']\n        treatments = sd['treatments']\n        bhl_pubs = sd['bhl_pubs']\n        \n        # Use pre-fetched Wikipedia intro\n        has_wiki = lang in taxon_langs.get(taxon_name, {})\n        wiki_intro = wiki_intros.get(lang, {}).get(taxon_name, '')\n        \n        # Write markdown page\n        page_path = f'{lang_dir}/{safe_name}.md'\n        toc_taxa_lang.append({'file': f'{lang_dir}/{safe_name}'})\n        \n        lines = []\n        \n        # Title\n        lines.append(f'# *{taxon_name}*')\n        lines.append('')\n        \n        # Wikidata / Scholia / GBIF link bar\n        lines.append(\n            f'[Wikidata ({qid})](https://www.wikidata.org/wiki/{qid}) \\u00b7 '\n            f'[Scholia](https://scholia.toolforge.org/taxon/{qid}) \\u00b7 '\n            f'[GBIF](https://www.gbif.org/species/search?q={taxon_name.replace(\" \", \"%20\")})'\n        )\n        lines.append('')\n        \n        # Wikipedia introduction\n        if wiki_intro:\n            lines.append('## About this species')\n            lines.append('')\n            lines.append(wiki_intro)\n            lines.append('')\n            article = taxon_langs[taxon_name][lang]\n            wp_url = f'https://{lang}.wikipedia.org/wiki/{article}'\n            lines.append(f'Read more on [{lang_name} Wikipedia]({wp_url}).')\n            lines.append('')\n        elif has_wiki:\n            article = taxon_langs[taxon_name][lang]\n            wp_url = f'https://{lang}.wikipedia.org/wiki/{article}'\n            lines.append(f'Read about this species on [{lang_name} Wikipedia]({wp_url}).')\n            lines.append('')\n        \n        # Other language editions\n        other_langs = {l for l in taxon_langs.get(taxon_name, {}).keys() if l != lang and l in active_langs}\n        if other_langs:\n            lang_links = []\n            for ol in sorted(other_langs):\n                article = taxon_langs[taxon_name][ol]\n                lang_links.append(f'[{get_lang_name(ol)}](https://{ol}.wikipedia.org/wiki/{article})')\n            lines.append('Also available in: ' + ' \\u00b7 '.join(lang_links))\n            lines.append('')\n        \n        # Map section\n        lines.append('## Global distribution')\n        lines.append('')\n        if occurrences:\n            lines.append(\n                f'Map shows **{len(occurrences)}** georeferenced GBIF records '\n                f'(out of {total_count} total) for *{taxon_name}* worldwide. '\n                'Click markers for details.'\n            )\n            lines.append('')\n            lines.append('```{raw} html')\n            lines.append(f'<iframe src=\"../maps/{safe_name}.html\" width=\"100%\" height=\"500px\" '\n                         'frameborder=\"0\" scrolling=\"no\" '\n                         'style=\"border-radius:8px; margin-bottom:1rem;\"></iframe>')\n            lines.append('```')\n            lines.append('')\n            lines.append(\n                'Map data \\u00a9 [OpenStreetMap](https://www.openstreetmap.org/copyright) contributors. '\n                'Occurrence data from [GBIF](https://www.gbif.org/).'\n            )\n        else:\n            lines.append('*No georeferenced occurrence records found in GBIF for this species.*')\n        lines.append('')\n        \n        # Plazi TreatmentBank section\n        if treatments:\n            lines.append('## Taxonomic treatments')\n            lines.append('')\n            lines.append('Taxonomic treatments from [Plazi TreatmentBank](https://plazi.org/treatmentbank/):')\n            lines.append('')\n            for tr in treatments:\n                tr_url = tr['treatment_page']\n                creator = tr['creator']\n                pub = tr['pub_title']\n                label_parts = []\n                if creator:\n                    label_parts.append(creator)\n                if pub:\n                    label_parts.append(f'*{pub}*')\n                label = ' \\u2014 '.join(label_parts) if label_parts else 'Treatment'\n                lines.append(f'- [{label}]({tr_url})')\n                lines.append('')\n            syno_url = f'https://synospecies.plazi.org/#{taxon_name.replace(\" \", \"%20\")}'\n            lines.append(f'[View all treatments for *{taxon_name}* on Synospecies]({syno_url})')\n            lines.append('')\n        \n        # BHL Literature section\n        if bhl_pubs:\n            lines.append('## Literature')\n            lines.append('')\n            lines.append('Publications from the [Biodiversity Heritage Library](https://www.biodiversitylibrary.org/):')\n            lines.append('')\n            for pub in bhl_pubs:\n                title = pub['title']\n                url = pub['bhl_url']\n                authors = pub['authors']\n                date = pub['date']\n                suffix_parts = []\n                if authors:\n                    suffix_parts.append(authors)\n                if date:\n                    suffix_parts.append(date)\n                suffix = ' \\u2014 ' + ', '.join(suffix_parts) if suffix_parts else ''\n                lines.append(f'- [{title}]({url}){suffix}')\n                lines.append('')\n            bhl_search = f'https://www.biodiversitylibrary.org/search?SearchTerm={taxon_name.replace(\" \", \"+\")}'\n            lines.append(f'[Search for *{taxon_name}* on BHL]({bhl_search})')\n            lines.append('')\n        \n        # Observations from Montserrat\n        lines.append('## Observations from Montserrat')\n        lines.append('')\n        lines.append('Observations from the GBIF dataset covering Montserrat grasses and sedges.')\n        lines.append('')\n        \n        for org_name, obs_dict in sorted(info['publishers'].items()):\n            lines.append(f'### {org_name}')\n            lines.append('')\n            for obs_id, obs_data in obs_dict.items():\n                license_label, license_url = resolve_license_label(obs_data['license'])\n                lines.append(f'**Observation:** [{obs_id}]({obs_id})  ')\n                lines.append(f'**License:** [{license_label}]({license_url})')\n                lines.append('')\n                for media_url in obs_data['media']:\n                    if media_url and media_url != 'nan':\n                        display_url = media_url.replace('square', 'medium')\n                        lines.append(f'![{taxon_name} \\u2014 {org_name}]({display_url})')\n                        lines.append('')\n        \n        with open(page_path, 'w', encoding='utf-8') as f:\n            f.write('\\n'.join(lines))\n    \n    print(f'  Generated {len(toc_taxa_lang)} pages in {lang_dir}/')\n\nprint(f'\\nAll language editions generated.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Generate Montserrat overview map + per-language configs\n\nGenerate the overview map (shared across all languages), then create\n`_config_{lang}.yml`, `_toc_{lang}.yml`, `intro_{lang}.md`, and the landing page."
  },
  {
   "cell_type": "code",
   "id": "4yzuq1mve39",
   "source": "# ---- Montserrat overview map (shared, language-independent) ----\n\ndf_occ = pd.read_csv(\n    'data/0002020-240626123714530/occurrence.txt',\n    sep='\\t', on_bad_lines='warn', low_memory=False\n)\n\ndf_coords = df_occ[\n    df_occ['decimalLatitude'].notna() &\n    df_occ['decimalLongitude'].notna() &\n    (df_occ['taxonRank'] == 'SPECIES')\n].copy()\n\nprint(f'Georeferenced records: {len(df_coords)} across {df_coords[\"species\"].nunique()} species')\n\nspecies_list = sorted(df_coords['species'].dropna().unique())\nn = len(species_list)\n\ndef hsl_to_hex(h, s, l):\n    r, g, b = colorsys.hls_to_rgb(h, l, s)\n    return '#{:02x}{:02x}{:02x}'.format(int(r*255), int(g*255), int(b*255))\n\nspecies_colours = {sp: hsl_to_hex(i / n, 0.75, 0.42) for i, sp in enumerate(species_list)}\n\noverview_map = folium.Map(\n    location=[16.745, -62.202], zoom_start=12,\n    tiles='OpenStreetMap', width='100%', height='500px'\n)\n\nfor _, row in df_coords.iterrows():\n    sp = row.get('species', '')\n    colour = species_colours.get(sp, '#888888')\n    gbif_link = 'https://www.gbif.org/occurrence/' + str(int(row['gbifID']))\n    year_str = str(int(row['year'])) if pd.notna(row.get('year')) else ''\n    inst_str = str(row.get('institutionCode', ''))\n    popup_html = (\n        f'<b><i>{sp}</i></b><br>{inst_str} {year_str}<br>'\n        f'<a href=\"{gbif_link}\" target=\"_blank\">GBIF record</a>'\n    )\n    folium.CircleMarker(\n        location=[row['decimalLatitude'], row['decimalLongitude']],\n        radius=7, color=colour, fill=True, fill_color=colour,\n        fill_opacity=0.85,\n        popup=folium.Popup(popup_html, max_width=220),\n        tooltip=f'<i>{sp}</i>'\n    ).add_to(overview_map)\n\nlegend_items = ''.join(\n    f'<span style=\"color:{c};\">&#9679;</span> <i>{sp}</i><br>'\n    for sp, c in sorted(species_colours.items())\n)\nlegend_html = (\n    '<div style=\"position:fixed; bottom:20px; left:20px; z-index:9999;'\n    ' background:white; padding:10px 14px; border-radius:6px;'\n    ' border:1px solid #ccc; font-size:11px; line-height:1.8;'\n    ' max-height:280px; overflow-y:auto;\">'\n    '<b>Species</b><br>' + legend_items + '</div>'\n)\noverview_map.get_root().html.add_child(folium.Element(legend_html))\noverview_map.save('maps/montserrat_overview.html')\nprint('Saved maps/montserrat_overview.html')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "04vzuhvuure6",
   "source": "# ---- Generate per-language _config, _toc, intro files ----\n\nfor lang in sorted(active_langs.keys()):\n    lang_name = get_lang_name(lang)\n    lang_dir = f'taxa_{lang}'\n    \n    # Build sorted taxa file list for this language\n    toc_taxa_sorted = sorted(\n        [{'file': f'{lang_dir}/{tn.replace(\" \", \"_\")}'} for tn in taxonpages.keys()],\n        key=lambda x: x['file']\n    )\n    \n    # Organisation chapters (shared across languages)\n    org_chapters = []\n    for fname in sorted(os.listdir('organisation')):\n        if fname.endswith('.md'):\n            org_chapters.append({'file': f'organisation/{fname[:-3]}'})\n    \n    # ---- _toc_{lang}.yml ----\n    toc_data = {\n        'format': 'jb-book',\n        'root': f'intro_{lang}',\n        'parts': [\n            {\n                'caption': 'Collections with grasses and sedges from Montserrat',\n                'chapters': org_chapters\n            },\n            {\n                'caption': 'Taxa originally from Montserrat',\n                'chapters': toc_taxa_sorted\n            }\n        ]\n    }\n    \n    toc_path = f'_toc_{lang}.yml'\n    with open(toc_path, 'w', encoding='utf-8') as f:\n        yaml.dump(toc_data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)\n    \n    # ---- _config_{lang}.yml ----\n    config_data = {\n        'title': f'Grasses and sedges of Montserrat ({lang_name})',\n        'author': 'Sofie Meeus, Quentin Groom, Andra Waagmeester',\n        'logo': 'logo.png',\n        'execute': {\n            'execute_notebooks': 'off'  # No execution — pages are pre-generated markdown\n        },\n        'latex': {\n            'latex_documents': {'targetname': 'book.tex'}\n        },\n        'bibtex_bibfiles': ['references.bib'],\n        'repository': {\n            'url': 'https://github.com/VisibleNatureAtlas/Grasses-and-sedges-of-Montserrat',\n            'path_to_book': '.',\n            'branch': 'main'\n        },\n        'html': {\n            'use_issues_button': True,\n            'use_repository_button': True,\n            'extra_navbar': 'Powered by <a href=\"https://jupyterbook.org\">Jupyter Book</a>',\n            'extra_footer': (\n                '<p>Data sourced from <a href=\"https://www.gbif.org/\">GBIF</a> and '\n                '<a href=\"https://www.wikidata.org/\">Wikidata</a> under open licenses. '\n                'Maps \\u00a9 <a href=\"https://www.openstreetmap.org/copyright\">OpenStreetMap</a> contributors.</p>'\n            )\n        },\n        'sphinx': {\n            'config': {\n                'html_show_copyright': False,\n                'exclude_patterns': ['node_modules', 'myenv', '_build', 'maps',\n                                     'GrassesMontserrat']\n            }\n        }\n    }\n    \n    # Also exclude other language taxa dirs to avoid confusion\n    for other_lang in active_langs:\n        if other_lang != lang:\n            config_data['sphinx']['config']['exclude_patterns'].append(f'taxa_{other_lang}')\n    # Exclude the old taxa/ dir too\n    config_data['sphinx']['config']['exclude_patterns'].append('taxa')\n    \n    config_path = f'_config_{lang}.yml'\n    with open(config_path, 'w', encoding='utf-8') as f:\n        yaml.dump(config_data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)\n    \n    # ---- intro_{lang}.md ----\n    # Count Wikipedia coverage for this language\n    n_with_wiki = len(active_langs[lang])\n    n_total = len(taxonpages)\n    \n    intro_lines = []\n    intro_lines.append(f'# Visible Nature Atlas: Grasses and sedges of Montserrat')\n    intro_lines.append('')\n    intro_lines.append(f'*{lang_name} edition* \\u2014 Species descriptions from [{lang_name} Wikipedia](https://{lang}.wikipedia.org/).')\n    intro_lines.append('')\n    intro_lines.append(\n        'This is a volume in the **Visible Nature Atlas** series \\u2014 an open, data-driven collection of '\n        'biodiversity books covering the flora and fauna of specific regions worldwide.'\n    )\n    intro_lines.append('')\n    \n    # Language picker\n    intro_lines.append('## Other language editions')\n    intro_lines.append('')\n    other_editions = []\n    for ol in sorted(active_langs.keys()):\n        if ol != lang:\n            ol_name = get_lang_name(ol)\n            n_ol = len(active_langs[ol])\n            other_editions.append(f'[{ol_name} ({n_ol} species)](../{ol}/index.html)')\n    if other_editions:\n        intro_lines.append(' \\u00b7 '.join(other_editions))\n        intro_lines.append('')\n    \n    intro_lines.append('## About this atlas')\n    intro_lines.append('')\n    intro_lines.append(\n        'This atlas documents the **grasses (Poaceae) and sedges (Cyperaceae)** of **Montserrat**, a small '\n        'British Overseas Territory in the Caribbean Lesser Antilles. Despite its modest size (102 km\\u00b2), '\n        'Montserrat harbours a rich diversity of native and introduced grass and sedge species.'\n    )\n    intro_lines.append('')\n    intro_lines.append(f'This edition includes Wikipedia descriptions for **{n_with_wiki}** out of {n_total} species.')\n    intro_lines.append('')\n    \n    # Overview map — intro is the root page, so maps/ is a sibling directory (no ../)\n    intro_lines.append('## Observations on Montserrat')\n    intro_lines.append('')\n    intro_lines.append('```{raw} html')\n    intro_lines.append('<iframe src=\"maps/montserrat_overview.html\"')\n    intro_lines.append('        width=\"100%\" height=\"520px\"')\n    intro_lines.append('        frameborder=\"0\" scrolling=\"no\"')\n    intro_lines.append('        style=\"border-radius:8px; margin-bottom:0.5rem;\"></iframe>')\n    intro_lines.append('```')\n    intro_lines.append('')\n    intro_lines.append(\n        'Map data \\u00a9 [OpenStreetMap](https://www.openstreetmap.org/copyright) contributors. '\n        'Occurrence data from [GBIF](https://www.gbif.org/).'\n    )\n    intro_lines.append('')\n    \n    # Wikipedia language availability table\n    intro_lines.append('## Wikipedia language availability')\n    intro_lines.append('')\n    intro_lines.append(\n        'The table below shows which species have Wikipedia articles in each language. '\n        'Click a language code to visit the Wikipedia article.'\n    )\n    intro_lines.append('')\n    intro_lines.append(wiki_table_md)\n    intro_lines.append('')\n    \n    # Data sources\n    intro_lines.append('## Data sources')\n    intro_lines.append('')\n    intro_lines.append('| Source | Role |')\n    intro_lines.append('|--------|------|')\n    intro_lines.append('| **[GBIF](https://www.gbif.org/)** | Occurrence records |')\n    intro_lines.append('| **[Wikidata](https://www.wikidata.org/)** | Taxonomic identifiers, metadata |')\n    intro_lines.append(f'| **[Wikipedia](https://{lang}.wikipedia.org/)** | Species descriptions |')\n    intro_lines.append('| **[Plazi TreatmentBank](https://plazi.org/)** | Taxonomic treatments |')\n    intro_lines.append('| **[BHL](https://www.biodiversitylibrary.org/)** | Historical literature |')\n    intro_lines.append('| **[OpenStreetMap](https://www.openstreetmap.org/)** | Map tiles |')\n    intro_lines.append('')\n    \n    intro_lines.append('## Authors')\n    intro_lines.append('')\n    intro_lines.append('- **Sofie Meeus** \\u2014 [Meise Botanic Garden](https://www.botanicgarden.be/)')\n    intro_lines.append('- **Quentin Groom** \\u2014 [Meise Botanic Garden](https://www.botanicgarden.be/)')\n    intro_lines.append('- **Andra Waagmeester** \\u2014 [Gene Wiki](https://www.wikidata.org/wiki/User:Andrawaag)')\n    intro_lines.append('')\n    intro_lines.append('```{note}')\n    intro_lines.append('The interactive maps require JavaScript to be enabled in your browser.')\n    intro_lines.append('```')\n    intro_lines.append('')\n    \n    intro_path = f'intro_{lang}.md'\n    with open(intro_path, 'w', encoding='utf-8') as f:\n        f.write('\\n'.join(intro_lines))\n    \n    print(f'{lang}: {config_path}, {toc_path}, {intro_path}')\n\nprint(f'\\nGenerated configs for {len(active_langs)} languages.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "13b8kaqctlf",
   "source": "# ---- Write .languages.txt for the build script ----\nsorted_langs_list = sorted(active_langs.keys())\nwith open('.languages.txt', 'w') as f:\n    f.write('\\n'.join(sorted_langs_list) + '\\n')\nprint(f'Wrote .languages.txt with {len(sorted_langs_list)} languages: {\", \".join(sorted_langs_list)}')\n\n# ---- Generate landing page (index.html) ----\n# This is a simple HTML page that links to each language edition.\n\nlang_cards = ''\nfor lang in sorted(sorted_langs_list, key=lambda l: (-len(active_langs[l]), l)):\n    name = get_lang_name(lang)\n    n = len(active_langs[lang])\n    total = len(taxonpages)\n    pct = round(100 * n / total) if total > 0 else 0\n    lang_cards += f'''\n        <a href=\"{lang}/index.html\" class=\"lang-card\">\n            <div class=\"lang-code\">{lang}</div>\n            <div class=\"lang-name\">{name}</div>\n            <div class=\"lang-stats\">{n}/{total} species with Wikipedia ({pct}%)</div>\n        </a>\n'''\n\nlanding_html = f'''<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Visible Nature Atlas: Grasses and sedges of Montserrat</title>\n    <style>\n        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n        body {{\n            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n            background: #f7f9fc;\n            color: #2c3e50;\n            min-height: 100vh;\n        }}\n        .hero {{\n            background: linear-gradient(135deg, #1a5632 0%, #2d8a56 50%, #4aa564 100%);\n            color: white;\n            padding: 3rem 2rem;\n            text-align: center;\n        }}\n        .hero h1 {{ font-size: 2.2rem; margin-bottom: 0.5rem; }}\n        .hero p {{ font-size: 1.1rem; opacity: 0.9; max-width: 600px; margin: 0 auto; }}\n        .container {{\n            max-width: 900px;\n            margin: 2rem auto;\n            padding: 0 1.5rem;\n        }}\n        h2 {{\n            font-size: 1.4rem;\n            margin-bottom: 1rem;\n            color: #2c3e50;\n        }}\n        .lang-grid {{\n            display: grid;\n            grid-template-columns: repeat(auto-fill, minmax(220px, 1fr));\n            gap: 1rem;\n            margin-bottom: 2rem;\n        }}\n        .lang-card {{\n            background: white;\n            border: 1px solid #e0e6ed;\n            border-radius: 8px;\n            padding: 1.2rem;\n            text-decoration: none;\n            color: inherit;\n            transition: transform 0.15s, box-shadow 0.15s;\n        }}\n        .lang-card:hover {{\n            transform: translateY(-2px);\n            box-shadow: 0 4px 12px rgba(0,0,0,0.1);\n        }}\n        .lang-code {{\n            font-size: 1.6rem;\n            font-weight: 700;\n            color: #1a5632;\n        }}\n        .lang-name {{\n            font-size: 1rem;\n            margin: 0.3rem 0;\n        }}\n        .lang-stats {{\n            font-size: 0.85rem;\n            color: #7f8c8d;\n        }}\n        .footer {{\n            text-align: center;\n            padding: 2rem;\n            font-size: 0.85rem;\n            color: #7f8c8d;\n        }}\n        .footer a {{ color: #1a5632; }}\n    </style>\n</head>\n<body>\n    <div class=\"hero\">\n        <h1>Grasses and sedges of Montserrat</h1>\n        <p>A multilingual biodiversity atlas from the Visible Nature Atlas series.\n           Choose a language edition below.</p>\n    </div>\n    <div class=\"container\">\n        <h2>Available editions ({len(sorted_langs_list)} languages)</h2>\n        <div class=\"lang-grid\">\n            {lang_cards}\n        </div>\n    </div>\n    <div class=\"footer\">\n        Data from <a href=\"https://www.gbif.org/\">GBIF</a>,\n        <a href=\"https://www.wikidata.org/\">Wikidata</a>,\n        <a href=\"https://en.wikipedia.org/\">Wikipedia</a>,\n        <a href=\"https://plazi.org/\">Plazi</a> &amp;\n        <a href=\"https://www.biodiversitylibrary.org/\">BHL</a>.\n        Maps &copy; <a href=\"https://www.openstreetmap.org/copyright\">OpenStreetMap</a> contributors.<br>\n        Built with <a href=\"https://jupyterbook.org/\">Jupyter Book</a>.\n        Source on <a href=\"https://github.com/VisibleNatureAtlas/Grasses-and-sedges-of-Montserrat\">GitHub</a>.\n    </div>\n</body>\n</html>'''\n\nwith open('_build_landing.html', 'w', encoding='utf-8') as f:\n    f.write(landing_html)\n\nprint(f'Landing page written to _build_landing.html')\nprint(f'\\nDone! Ready to build {len(sorted_langs_list)} language editions.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}